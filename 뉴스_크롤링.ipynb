{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 뉴스 크롤링"
      ],
      "metadata": {
        "id": "oqCEU2y6LZXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "id": "tENVRECcAANQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver-manager"
      ],
      "metadata": {
        "id": "eijBgLNJEs7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "id": "utuWY_1GBQso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "KST = timezone('Asia/Seoul')\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome(options=options)"
      ],
      "metadata": {
        "id": "hdoJPlf8A9K1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기사 링크 크롤링"
      ],
      "metadata": {
        "id": "ooToJ_47sTz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "H7_L6uD247Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48d2515-b66a-4278-e9ef-d2cdd2b977af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생활/문화 1 페이지\n",
            "연예 1 페이지\n"
          ]
        }
      ],
      "source": [
        "# 기사 링크 크롤링\n",
        "# 카테고리당 100개 = 5페이지\n",
        "\n",
        "six_url_list = []   # url 저장 리스트\n",
        "entertainment_url_list = []\n",
        "sports_url_list = []\n",
        "\n",
        "category_list = []\n",
        "categoty_name = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\"]\n",
        "\n",
        "def getSixLink():    # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "    a_list = []\n",
        "\n",
        "    for category in range(3, 4):     # 6\n",
        "        for page in range(1, 2):  # 1, 6\n",
        "            url = f'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={100 + category}#&date=%2000:00:00&page={page}'\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "            a_list.extend(soup.select(\".type06_headline dt+dt a\"))\n",
        "            a_list.extend(soup.select(\".type06 dt+dt a\"))\n",
        "\n",
        "            category_list.extend([categoty_name[category] for _ in range(20)])\n",
        "            print(f\"{categoty_name[category]} {page} 페이지\")\n",
        "\n",
        "    for a in a_list:\n",
        "        six_url_list.append(a[\"href\"])\n",
        "\n",
        "\n",
        "def getEntertainmentLink():   # 연예\n",
        "    # today = str(datetime.datetime.now(KST))[:11]  # 서울 기준 시간\n",
        "    a_list = []\n",
        "    today = datetime.date.today()\n",
        "\n",
        "    for page in range(1, 2):  # 1, 5\n",
        "        url = f'https://entertain.naver.com/now#sid=106&date={today}&page={page}'\n",
        "        browser.get(url)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "        a_list.extend(soup.select(\".news_lst li>a\"))\n",
        "\n",
        "        category_list.extend([\"연예\" for _ in range(25)])\n",
        "        print(f\"연예 {page} 페이지\")\n",
        "\n",
        "    for a in a_list:\n",
        "        entertainment_url_list.append(\"https://entertain.naver.com\" + a[\"href\"])\n",
        "\n",
        "\n",
        "def getSportsLink():    # 스포츠  (페이지마다 개수가 달라서 6페이지를 이동)\n",
        "    # today = str(datetime.datetime.now(KST))[:11].replace('-', '')  # 서울 기준 시간\n",
        "    a_list = []\n",
        "    today = str(datetime.date.today()).replace('-', '')\n",
        "\n",
        "    for page in range(1, 2):  # 1, 7\n",
        "        url = f'https://sports.news.naver.com/general/news/index?isphoto=N&type=latest&date={today}&page={page}'\n",
        "        browser.get(url)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "        a_list.extend(soup.select(\".news_list li>a\"))\n",
        "\n",
        "        print(f\"스포츠 {page} 페이지\")\n",
        "\n",
        "    for i in range(len(a_list)):\n",
        "        if i == 100:  # 100개 링크 추가했으면 멈추기\n",
        "            break\n",
        "        sports_url_list.append(\"https://sports.news.naver.com/news\" + re.search('\\?.+', a_list[i][\"href\"]).group())\n",
        "        category_list.extend([\"스포츠\"])\n",
        "\n",
        "getSixLink()\n",
        "getEntertainmentLink()\n",
        "# getSportsLink()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(six_url_list))\n",
        "print(len(entertainment_url_list))\n",
        "print(len(sports_url_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soRRxGwpFW0y",
        "outputId": "89751b0c-a4fd-451b-b5d5-1d99d88d47ab"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "25\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 제목, 본문, 날짜 크롤링"
      ],
      "metadata": {
        "id": "jRtY5J-dso7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집한 태그 속에서 내용만 뽑아서 담는다.\n",
        "\n",
        "def clean(article):   # 전처리\n",
        "    article = re.sub('\\w+ 기자','',article)\n",
        "    article = re.sub('\\w{2,4} 온라인 기자','',article)\n",
        "    article = re.sub('\\w{2,4}기자','',article)\n",
        "    # article = re.sub('\\S+@[a-z.]+','',article)\n",
        "\n",
        "    article = re.sub('\\n','',article)\n",
        "    article = re.sub('\\t','',article)\n",
        "    article = re.sub('\\u200b','',article)\n",
        "    article = re.sub('\\xa0','',article)\n",
        "    article = re.sub('([a-zA-Z])','',article)\n",
        "    article = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',article)\n",
        "    article = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',article)\n",
        "    article = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',article)\n",
        "\n",
        "\n",
        "    return article\n"
      ],
      "metadata": {
        "id": "wORd2Lf8KTiT"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집한 링크의 제목, 본문, 날짜 크롤링\n",
        "\n",
        "title_list = []     # 제목\n",
        "content_list = []   # 본문\n",
        "date_list = []      # 날짜\n",
        "\n",
        "def getSixContent(url_list):   # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "    title = []\n",
        "    content = []\n",
        "    date = []\n",
        "\n",
        "    for url in url_list:\n",
        "        browser.get(url)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "        title.extend(soup.select(\"#title_area span\"))                 # 제목\n",
        "\n",
        "        c = soup.find_all(attrs={\"id\" : \"dic_area\"})                  # 본문\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"end_photo_org\"}):           # 이미지 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"vod_player_wrap\"}):         # 영상 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"vod_player_wrap\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "        content.extend(c)\n",
        "\n",
        "        date.extend(soup.select(\"._ARTICLE_DATE_TIME\"))               # 날짜\n",
        "\n",
        "    for t in title:\n",
        "        title_list.append(clean(t.text))\n",
        "\n",
        "    for c in content:\n",
        "        content_list.append(clean(c.text))\n",
        "\n",
        "    for d in date:\n",
        "        date_list.append(d[\"data-date-time\"])\n",
        "\n",
        "\n",
        "def getEntertainmentContent(url_list):    # 연예\n",
        "    title = []\n",
        "    content = []\n",
        "    date = []\n",
        "\n",
        "    for url in url_list:\n",
        "        browser.get(url)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "        title.extend(soup.select(\".end_tit\"))                 # 제목\n",
        "\n",
        "        c = soup.find_all(attrs={\"id\" : \"articeBody\"})                # 본문\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"end_photo_org\"}):           # 이미지 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "          if c[0].find(attrs={\"class\" : \"caption\"}):                  # 이미지 설명 없애기\n",
        "            c[0].find(attrs={\"class\" : \"caption\"}).decompose()\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"video_area\"}):         # 영상 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"video_area\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "        while c[0].find(attrs={\"name\" : \"iframe\"}):\n",
        "           c[0].find(attrs={\"name\" : \"iframe\"}).decompose()\n",
        "\n",
        "        content.extend(c)\n",
        "\n",
        "        date.extend(soup.select_one(\".author em\"))               # 날짜\n",
        "\n",
        "    for t in title:\n",
        "      title_list.append(clean(t.text))\n",
        "\n",
        "    for c in content:\n",
        "      content_list.append(clean(c.text))\n",
        "\n",
        "    for d in date:\n",
        "      date_list.append(d.text)\n",
        "\n",
        "\n",
        "def getSportsContent(url_list):   # 스포츠\n",
        "    title = []\n",
        "    content = []\n",
        "    date = []\n",
        "\n",
        "    for url in url_list:\n",
        "        print(url)\n",
        "        browser.get(url)\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "        title.extend(soup.select(\".title\"))                 # 제목\n",
        "\n",
        "        c = soup.find_all(attrs={\"class\" : \"news_end\"})                # 본문\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"end_photo_org\"}):           # 이미지 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"image\"}):\n",
        "           c[0].find(attrs={\"class\" : \"image\"}).decompose()\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"vod_area\"}):         # 영상 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"vod_area\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "        content.extend(c)\n",
        "\n",
        "        date.extend(soup.select_one(\".info span\"))               # 날짜\n",
        "\n",
        "    for t in title:\n",
        "      title_list.append(clean(t.text))\n",
        "\n",
        "    for c in content:\n",
        "      content_list.append(clean(c.text))\n",
        "\n",
        "    for d in date:\n",
        "      date_list.append(d.text)\n",
        "\n",
        "getSixContent(six_url_list)\n",
        "getEntertainmentContent(entertainment_url_list)\n",
        "# getSportsContent(sports_url_list)\n"
      ],
      "metadata": {
        "id": "W19FqHMx1lg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb65974-b93f-4a77-c26c-5ef23c5a2cf3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://n.news.naver.com/mnews/article/001/0014305260?sid=103\n",
            "https://n.news.naver.com/mnews/article/056/0011594468?sid=103\n",
            "https://n.news.naver.com/mnews/article/001/0014305246?sid=103\n",
            "https://n.news.naver.com/mnews/article/079/0003829552?sid=103\n",
            "https://n.news.naver.com/mnews/article/032/0003258784?sid=103\n",
            "https://n.news.naver.com/mnews/article/296/0000071064?sid=103\n",
            "https://n.news.naver.com/mnews/article/018/0005610178?sid=103\n",
            "https://n.news.naver.com/mnews/article/052/0001954797?sid=103\n",
            "https://n.news.naver.com/mnews/article/022/0003870932?sid=103\n",
            "https://n.news.naver.com/mnews/article/422/0000627197?sid=103\n",
            "https://n.news.naver.com/mnews/article/088/0000843853?sid=103\n",
            "https://n.news.naver.com/mnews/article/657/0000019914?sid=103\n",
            "https://n.news.naver.com/mnews/article/056/0011594418?sid=103\n",
            "https://n.news.naver.com/mnews/article/009/0005208710?sid=103\n",
            "https://n.news.naver.com/mnews/article/032/0003258776?sid=103\n",
            "https://n.news.naver.com/mnews/article/003/0012184424?sid=103\n",
            "https://n.news.naver.com/mnews/article/025/0003318759?sid=103\n",
            "https://n.news.naver.com/mnews/article/032/0003258773?sid=103\n",
            "https://n.news.naver.com/mnews/article/092/0002310030?sid=103\n",
            "https://n.news.naver.com/mnews/article/003/0012184393?sid=103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(title_list))\n",
        "print(len(content_list))\n",
        "print(len(date_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Qxk3prZjxk",
        "outputId": "f212d825-2976-4582-d47b-fd402dab82dd"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "45\n",
            "45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 프레임 생성"
      ],
      "metadata": {
        "id": "zGI8z-6Et67j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임으로 변환\n",
        "all_url_list = six_url_list + entertainment_url_list + sports_url_list\n",
        "\n",
        "article_df = pd.DataFrame({\"카테고리\" : category_list,\n",
        "                           \"날짜\" : date_list,\n",
        "                           \"제목\" : title_list,\n",
        "                           \"본문\" : content_list,\n",
        "                           \"링크\" : all_url_list})\n",
        "\n",
        "article_df"
      ],
      "metadata": {
        "id": "5ZgZ_-VN9mGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "OUvMLvNWLhp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoNLPy**\n",
        "- 한국어 정보처리를 위한 파이썬 패키지\n",
        "- https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class"
      ],
      "metadata": {
        "id": "1q3t3HMTMmH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "QR83KuOIMYhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# CountVectorizer : 텍스트 빈도만을 카운트해서 벡터화\n",
        "# TfidfVectorizer : 자주 나오는 단어에 높은 가중치, 모든 문서에서 자주 나오는 단어에 패널티"
      ],
      "metadata": {
        "id": "xzW29PkHLlH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "nouns_list = []   # 명사 리스트\n",
        "\n",
        "for content in article_df[\"본문\"]:\n",
        "  nouns_list.append(okt.nouns(content))     # 명사 추출, 리스트 반환\n",
        "\n",
        "article_df[\"명사\"] = nouns_list   # 데이터 프레임에 추가"
      ],
      "metadata": {
        "id": "zMbU268wP0lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\" \".join(noun) for noun in article_df[\"명사\"]]  # 명사 열을 하나의 리스트에 담는다.\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df = 5, ngram_range=(1, 5))"
      ],
      "metadata": {
        "id": "5LuGfB9uUv-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 군집화"
      ],
      "metadata": {
        "id": "-s72jMBFSYAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "# DBSCAN : 밀도 기반 클러스터링\n",
        "# https://hoonzi-text.tistory.com/19"
      ],
      "metadata": {
        "id": "8iBDg5xASZlb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}