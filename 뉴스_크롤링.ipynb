{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 뉴스 크롤링"
      ],
      "metadata": {
        "id": "oqCEU2y6LZXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "id": "tENVRECcAANQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver-manager"
      ],
      "metadata": {
        "id": "eijBgLNJEs7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "id": "utuWY_1GBQso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "KST = timezone('Asia/Seoul')\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome(options=options)"
      ],
      "metadata": {
        "id": "hdoJPlf8A9K1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기사 링크 크롤링"
      ],
      "metadata": {
        "id": "ooToJ_47sTz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "H7_L6uD247Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d01424-817c-4928-b288-0335926701af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연예 1 페이지\n"
          ]
        }
      ],
      "source": [
        "# 기사 링크 크롤링\n",
        "# 카테고리당 100개 = 5페이지\n",
        "\n",
        "url_list = []   # url 저장 리스트\n",
        "\n",
        "category_list = []\n",
        "categoty_name = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\"]\n",
        "\n",
        "def getLink():    # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "  a_list = []\n",
        "\n",
        "  for category in range(6):     # 6\n",
        "      for page in range(1, 6):  # 1, 6\n",
        "          url = f'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={100 + category}#&date=%2000:00:00&page={page}'\n",
        "          browser.get(url)\n",
        "\n",
        "          time.sleep(1)\n",
        "\n",
        "          soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "          a_list.extend(soup.select(\".type06_headline dt+dt a\"))\n",
        "          a_list.extend(soup.select(\".type06 dt+dt a\"))\n",
        "\n",
        "          category_list.extend([categoty_name[category] for _ in range(20)])\n",
        "          print(f\"{categoty_name[category]} {page} 페이지\")\n",
        "\n",
        "  for a in a_list:\n",
        "    url_list.append(a[\"href\"])\n",
        "\n",
        "\n",
        "def getEntertainmentLink():   # 연예\n",
        "  # today = str(datetime.datetime.now(KST))[:11]  # 서울 기준 시간\n",
        "  a_list = []\n",
        "  today = datetime.date.today()\n",
        "\n",
        "  for page in range(1, 2):  # 1, 5\n",
        "      url = f'https://entertain.naver.com/now#sid=106&date={today}&page={page}'\n",
        "      browser.get(url)\n",
        "\n",
        "      time.sleep(1)\n",
        "\n",
        "      soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "      a_list.extend(soup.select(\".news_lst li>a\"))\n",
        "\n",
        "      category_list.extend([\"연예\" for _ in range(25)])\n",
        "      print(f\"연예 {page} 페이지\")\n",
        "\n",
        "  for a in a_list:\n",
        "    url_list.append(\"https://entertain.naver.com\" + a[\"href\"])\n",
        "\n",
        "\n",
        "def getSportsLink():    # 스포츠  (페이지마다 개수가 달라서 6페이지를 이동)\n",
        "  # today = str(datetime.datetime.now(KST))[:11].replace('-', '')  # 서울 기준 시간\n",
        "  a_list = []\n",
        "  today = str(datetime.date.today()).replace('-', '')\n",
        "  for page in range(1, 7):  # 1, 7\n",
        "      url = f'https://sports.news.naver.com/general/news/index?isphoto=N&type=latest&date={today}&page={page}'\n",
        "      browser.get(url)\n",
        "\n",
        "      time.sleep(1)\n",
        "\n",
        "      soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "      a_list.extend(soup.select(\".news_list li>a\"))\n",
        "\n",
        "      print(f\"스포츠 {page} 페이지\")\n",
        "\n",
        "  for i in range(len(a_list)):\n",
        "    if i == 100:  # 100개 링크 추가했으면 멈추기\n",
        "      break\n",
        "    url_list.append(\"https://sports.news.naver.com/news\" + re.search('\\?.+', a_list[i][\"href\"]).group())\n",
        "    category_list.extend([\"스포츠\"])\n",
        "\n",
        "# getLink()\n",
        "getEntertainmentLink()\n",
        "# getSportsLink()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(url_list))\n",
        "print(len(category_list))\n",
        "print(url_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soRRxGwpFW0y",
        "outputId": "e8669f20-f5c6-4d0f-e436-824f87df1b71"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "25\n",
            "['https://entertain.naver.com/now/read?oid=109&aid=0004957664', 'https://entertain.naver.com/now/read?oid=241&aid=0003309895', 'https://entertain.naver.com/now/read?oid=108&aid=0003191894', 'https://entertain.naver.com/now/read?oid=640&aid=0000045305', 'https://entertain.naver.com/now/read?oid=109&aid=0004957663', 'https://entertain.naver.com/now/read?oid=241&aid=0003309894', 'https://entertain.naver.com/now/read?oid=241&aid=0003309893', 'https://entertain.naver.com/now/read?oid=468&aid=0000996931', 'https://entertain.naver.com/now/read?oid=144&aid=0000922403', 'https://entertain.naver.com/now/read?oid=108&aid=0003191892', 'https://entertain.naver.com/now/read?oid=109&aid=0004957662', 'https://entertain.naver.com/now/read?oid=640&aid=0000045303', 'https://entertain.naver.com/now/read?oid=241&aid=0003309892', 'https://entertain.naver.com/now/read?oid=421&aid=0007148458', 'https://entertain.naver.com/now/read?oid=311&aid=0001656566', 'https://entertain.naver.com/now/read?oid=109&aid=0004957661', 'https://entertain.naver.com/now/read?oid=382&aid=0001084989', 'https://entertain.naver.com/now/read?oid=396&aid=0000659207', 'https://entertain.naver.com/now/read?oid=311&aid=0001656564', 'https://entertain.naver.com/now/read?oid=144&aid=0000922402', 'https://entertain.naver.com/now/read?oid=241&aid=0003309891', 'https://entertain.naver.com/now/read?oid=241&aid=0003309890', 'https://entertain.naver.com/now/read?oid=076&aid=0004073277', 'https://entertain.naver.com/now/read?oid=003&aid=0012183264', 'https://entertain.naver.com/now/read?oid=076&aid=0004073276']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 제목, 본문, 날짜 크롤링"
      ],
      "metadata": {
        "id": "jRtY5J-dso7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집한 태그 속에서 내용만 뽑아서 담는다.\n",
        "\n",
        "def clean(article):   # 전처리\n",
        "  article = re.sub('\\n','',article)\n",
        "  article = re.sub('\\t','',article)\n",
        "  article = re.sub('\\u200b','',article)\n",
        "  article = re.sub('\\xa0','',article)\n",
        "  article = re.sub('([a-zA-Z])','',article)\n",
        "  article = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',article)\n",
        "  article = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',article)\n",
        "  article = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',article)\n",
        "  article = re.sub('.{2,3} 기자','',article)\n",
        "\n",
        "\n",
        "  return article\n"
      ],
      "metadata": {
        "id": "wORd2Lf8KTiT"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집한 링크의 제목, 본문, 날짜 크롤링\n",
        "\n",
        "title_list = []     # 제목\n",
        "content_list = []   # 본문\n",
        "date_list = []      # 날짜\n",
        "\n",
        "def getContent():   # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "  title = []\n",
        "  content = []\n",
        "  date = []\n",
        "\n",
        "  for url in url_list:\n",
        "        browser.get(url)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "        title.extend(soup.select(\"#title_area span\"))                 # 제목\n",
        "\n",
        "        c = soup.find_all(attrs={\"id\" : \"dic_area\"})                  # 본문\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"end_photo_org\"}):           # 이미지 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"vod_player_wrap\"}):         # 영상 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"vod_player_wrap\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "        content.extend(c)\n",
        "\n",
        "        date.extend(soup.select(\"._ARTICLE_DATE_TIME\"))               # 날짜\n",
        "\n",
        "  for t in title:\n",
        "    title_list.append(t.text)\n",
        "\n",
        "  for c in content:\n",
        "    content_list.append(clean(c.text))\n",
        "\n",
        "  for d in date:\n",
        "    date_list.append(d[\"data-date-time\"])\n",
        "\n",
        "\n",
        "def getEntertainmentContent():    # 연예\n",
        "    title = []\n",
        "    content = []\n",
        "    date = []\n",
        "\n",
        "    for url in url_list:\n",
        "        browser.get(url)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "        title.extend(soup.select(\".end_tit\"))                 # 제목\n",
        "\n",
        "        c = soup.find_all(attrs={\"id\" : \"articeBody\"})                # 본문\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"end_photo_org\"}):           # 이미지 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "          if c[0].find(attrs={\"class\" : \"caption\"}):                  # 이미지 설명 없애기\n",
        "            c[0].find(attrs={\"class\" : \"caption\"}).decompose()\n",
        "\n",
        "        while c[0].find(attrs={\"class\" : \"video_area\"}):         # 영상 있는 만큼\n",
        "          c[0].find(attrs={\"class\" : \"video_area\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "        while c[0].find(attrs={\"name\" : \"iframe\"}):\n",
        "           c[0].find(attrs={\"name\" : \"iframe\"}).decompose()\n",
        "\n",
        "        content.extend(c)\n",
        "\n",
        "        date.extend(soup.select_one(\".author em\"))               # 날짜\n",
        "\n",
        "    for t in title:\n",
        "      title_list.append(clean(t.text))\n",
        "\n",
        "    for c in content:\n",
        "      content_list.append(clean(c.text))\n",
        "\n",
        "    for d in date:\n",
        "      date_list.append(d.text)\n",
        "\n",
        "\n",
        "def getSportsContent():   # 스포츠\n",
        "  pass\n",
        "\n",
        "# getContent()\n",
        "getEntertainmentContent()\n"
      ],
      "metadata": {
        "id": "W19FqHMx1lg4"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(title_list))\n",
        "print(len(content_list))\n",
        "print(len(date_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Qxk3prZjxk",
        "outputId": "507262ce-f600-464c-a425-2ee929b22277"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "25\n",
            "25\n",
            "['2023.11.01. 오후 7:40', '2023.11.01. 오후 7:38', '2023.11.01. 오후 7:38', '2023.11.01. 오후 7:38', '2023.11.01. 오후 7:38', '2023.11.01. 오후 7:37', '2023.11.01. 오후 7:37', '2023.11.01. 오후 7:37', '2023.11.01. 오후 7:37', '2023.11.01. 오후 7:36', '2023.11.01. 오후 7:36', '2023.11.01. 오후 7:35', '2023.11.01. 오후 7:34', '2023.11.01. 오후 7:34', '2023.11.01. 오후 7:34', '2023.11.01. 오후 7:33', '2023.11.01. 오후 7:32', '2023.11.01. 오후 7:32', '2023.11.01. 오후 7:31', '2023.11.01. 오후 7:31', '2023.11.01. 오후 7:30', '2023.11.01. 오후 7:30', '2023.11.01. 오후 7:29', '2023.11.01. 오후 7:29', '2023.11.01. 오후 7:27']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 프레임 생성"
      ],
      "metadata": {
        "id": "zGI8z-6Et67j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임으로 변환\n",
        "\n",
        "article_df = pd.DataFrame({\"카테고리\" : category_list,\n",
        "                           \"날짜\" : date_list,\n",
        "                           \"제목\" : title_list,\n",
        "                           \"본문\" : content_list,\n",
        "                           \"링크\" : url_list})\n",
        "\n",
        "article_df"
      ],
      "metadata": {
        "id": "5ZgZ_-VN9mGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "OUvMLvNWLhp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoNLPy**\n",
        "- 한국어 정보처리를 위한 파이썬 패키지\n",
        "- https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class"
      ],
      "metadata": {
        "id": "1q3t3HMTMmH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "QR83KuOIMYhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# CountVectorizer : 텍스트 빈도만을 카운트해서 벡터화\n",
        "# TfidfVectorizer : 자주 나오는 단어에 높은 가중치, 모든 문서에서 자주 나오는 단어에 패널티"
      ],
      "metadata": {
        "id": "xzW29PkHLlH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "nouns_list = []   # 명사 리스트\n",
        "\n",
        "for content in article_df[\"본문\"]:\n",
        "  nouns_list.append(okt.nouns(content))     # 명사 추출, 리스트 반환\n",
        "\n",
        "article_df[\"명사\"] = nouns_list   # 데이터 프레임에 추가"
      ],
      "metadata": {
        "id": "zMbU268wP0lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\" \".join(noun) for noun in article_df[\"명사\"]]  # 명사 열을 하나의 리스트에 담는다.\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df = 5, ngram_range=(1, 5))"
      ],
      "metadata": {
        "id": "5LuGfB9uUv-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 군집화"
      ],
      "metadata": {
        "id": "-s72jMBFSYAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "# DBSCAN : 밀도 기반 클러스터링\n",
        "# https://hoonzi-text.tistory.com/19"
      ],
      "metadata": {
        "id": "8iBDg5xASZlb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}