{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MWaNV4H4wdNv"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# TfidfVectorizer : 자주 나오는 단어에 높은 가중치, 모든 문서에서 자주 나오는 단어에 패널티\n",
        "# DBSCAN : 밀도 기반 클러스터링 : 점 p에서 부터 거리 e (epsilon)내에 점이 m(minPts) 개 있으면 하나의 군집으로 인식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hdoJPlf8A9K1"
      },
      "outputs": [],
      "source": [
        "# KST = timezone('Asia/Seoul')    # 서울 시간\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome(options=options)\n",
        "# browser.implicitly_wait(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPDqkOU1laIT"
      },
      "source": [
        "# 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wORd2Lf8KTiT"
      },
      "outputs": [],
      "source": [
        "# 필요없는 내용 삭제 함수\n",
        "def clean(article):\n",
        "    article = re.sub('\\[.{1,15}\\]','',article)\n",
        "    article = re.sub('\\w{2,4} 온라인 기자','',article)\n",
        "    article = re.sub('\\w+ 기자','',article)\n",
        "    article = re.sub('\\w{2,4}기자','',article)\n",
        "    article = re.sub('\\w+ 기상캐스터','',article)\n",
        "    article = re.sub('사진','',article)\n",
        "    article = re.sub('포토','',article)\n",
        "    article = re.sub('\\(.*뉴스.{0,3}\\)','', article)  # (~뉴스~) 삭제\n",
        "    article = re.sub('\\S+@[a-z.]+','',article)          # 이메일 삭제\n",
        "\n",
        "    article = re.sub('\\n','',article)\n",
        "    article = re.sub('\\t','',article)\n",
        "    article = re.sub('\\u200b','',article)\n",
        "    article = re.sub('\\xa0','',article)\n",
        "    article = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',article)\n",
        "    # article = re.sub('([a-zA-Z])','',article)   # 영어 삭제\n",
        "    # article = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘’“”|\\(\\)\\[\\]\\<\\>`\\'…》]','',article)   # 특수문자 삭제\n",
        "\n",
        "    return article\n",
        "\n",
        "\n",
        "# 본문에서 명사 뽑아내는 함수\n",
        "def getNouns(article_df):\n",
        "    okt = Okt()\n",
        "    nouns_list = []                               # 명사 리스트\n",
        "\n",
        "    for content in article_df[\"content\"]:\n",
        "        nouns_list.append(okt.nouns(content))     # 명사 추출 (리스트 반환)\n",
        "\n",
        "    article_df[\"nouns\"] = nouns_list              # 데이터 프레임에 추가\n",
        "\n",
        "    return article_df\n",
        "\n",
        "# 명사를 벡터화 하는 함수\n",
        "def getVector(article_df):    # 카테고리 별로 벡터 생성\n",
        "    category_names = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\", \"연예\", \"스포츠\"]\n",
        "    vector_list = []\n",
        "\n",
        "    for i in range(8):\n",
        "        text = [\" \".join(noun) for noun in article_df['nouns'][article_df['category'] == category_names[i]]]    # 명사 열을 하나의 리스트에 담는다.\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer(min_df = 3, ngram_range=(1, 5))\n",
        "        tfidf_vectorizer.fit(text)\n",
        "        vector = tfidf_vectorizer.transform(text).toarray()                         # vector list 반환\n",
        "        vector = np.array(vector)\n",
        "        vector_list.append(vector)\n",
        "\n",
        "    return vector_list\n",
        "\n",
        "def convertCategory(article_df):    # 이름으로된 카테고리를 번호로 변환\n",
        "    category = [(\"정치\", \"100\"), (\"경제\", \"101\"), (\"사회\", \"102\"), (\"생활/문화\", \"103\"), (\"세계\", \"104\"), (\"IT/과학\", \"105\"), (\"연예\", \"106\"), (\"스포츠\", \"107\")]\n",
        "\n",
        "    for name, num in category:\n",
        "        article_df[\"category\"][article_df[\"category\"] == name] = num\n",
        "\n",
        "    return article_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooToJ_47sTz9"
      },
      "source": [
        "## 크롤링 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "VcCupDMKM9BX"
      },
      "outputs": [],
      "source": [
        "# 기사 링크 크롤링\n",
        "class UrlCrawling:\n",
        "    def __init__(self):\n",
        "        self.category_names = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\", \"연예\", \"스포츠\"]\n",
        "        self.category = []\n",
        "\n",
        "    def getSixUrl(self):    # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "        six_url = []\n",
        "        for category in range(1):     # 6\n",
        "            a_list = []\n",
        "            for page in range(1, 2):  # 1, 6\n",
        "                url = f'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={100 + category}#&date=%2000:00:00&page={page}'\n",
        "                browser.get(url)\n",
        "\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "                a_list.extend(soup.select(\".type06_headline dt+dt a\"))\n",
        "                a_list.extend(soup.select(\".type06 dt+dt a\"))\n",
        "\n",
        "                print(f\"{self.category_names[category]} {page} 페이지\")\n",
        "\n",
        "            for a in a_list:\n",
        "                six_url.append(a[\"href\"])\n",
        "                self.category.append(self.category_names[category])\n",
        "\n",
        "        return six_url\n",
        "\n",
        "\n",
        "    def getEntertainmentUrl(self):   # 연예\n",
        "        # today = str(datetime.datetime.now(KST))[:11]  # 서울 기준 시간\n",
        "        entertainment_url = []\n",
        "        a_list = []\n",
        "        today = datetime.date.today()\n",
        "\n",
        "        for page in range(1, 2):  # 1, 5\n",
        "            url = f'https://entertain.naver.com/now#sid=106&date={today}&page={page}'\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            a_list.extend(soup.select(\".news_lst li>a\"))\n",
        "\n",
        "\n",
        "            print(f\"연예 {page} 페이지\")\n",
        "\n",
        "        for a in a_list:\n",
        "            entertainment_url.append(\"https://entertain.naver.com\" + a[\"href\"])\n",
        "            self.category.append(\"연예\")\n",
        "\n",
        "        return entertainment_url\n",
        "\n",
        "    def getSportsUrl(self):    # 스포츠  (페이지마다 개수가 달라서 6페이지를 이동)\n",
        "        # today = str(datetime.datetime.now(KST))[:11].replace('-', '')  # 서울 기준 시간\n",
        "        sports_url = []\n",
        "        a_list = []\n",
        "        today = str(datetime.date.today()).replace('-', '')\n",
        "\n",
        "        for page in range(1, 2):  # 1, 7\n",
        "            url = f'https://sports.news.naver.com/general/news/index?isphoto=N&type=latest&date={today}&page={page}'\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "            a_list.extend(soup.select(\".news_list li>a\"))\n",
        "\n",
        "            print(f\"스포츠 {page} 페이지\")\n",
        "\n",
        "        for i in range(len(a_list)):\n",
        "            if i == 100:  # 100개 링크 추가했으면 멈추기\n",
        "                break\n",
        "            sports_url.append(\"https://sports.news.naver.com/news\" + re.search('\\?.+', a_list[i][\"href\"]).group())\n",
        "            self.category.append(\"스포츠\")\n",
        "\n",
        "        return sports_url\n",
        "\n",
        "\n",
        "\n",
        "# 기사 본문 크롤링\n",
        "class ContentCrawling:\n",
        "    def __init__(self, title, content, date, img):\n",
        "        self.title = title\n",
        "        self.content = content\n",
        "        self.date = date\n",
        "        self.img = img\n",
        "\n",
        "    def getSixContent(self, url_list):  # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "        title_list = []\n",
        "        content_list = []\n",
        "        date_list = []\n",
        "        img_list = []\n",
        "        cnt = 1\n",
        "\n",
        "        for url in url_list:\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            print(cnt)\n",
        "            cnt+=1\n",
        "\n",
        "            try:\n",
        "                title_list.extend(soup.select(\"#title_area span\"))              # 제목 추가\n",
        "\n",
        "                c = soup.find_all(attrs={\"id\" : \"dic_area\"})                    # 본문 가져오기\n",
        "\n",
        "                img_tag = soup.select(\".end_photo_org img\")                     # 이미지 가져오기\n",
        "\n",
        "                if img_tag:                                                     # 이미지 있으면 이미지 주소만 추출해서 리스트로 만든다.\n",
        "                    img_src_list = []\n",
        "                    for img in img_tag:\n",
        "                        img_src_list.append(img['src'])\n",
        "                    img_list.append(img_src_list)\n",
        "                else:\n",
        "                    img_list.append([])\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"end_photo_org\"}):             # 이미지 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"vod_player_wrap\"}):           # 영상 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"vod_player_wrap\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "                if c[0].find(attrs={\"class\" : \"artical-btm\"}):                  # 하단에 제보하기 칸 있으면 삭제\n",
        "                    c[0].find(attrs={\"class\" : \"artical-btm\"}).decompose()\n",
        "\n",
        "                content_list.extend(c)                                          # 본문 추가\n",
        "\n",
        "                date_list.extend(soup.select(\"._ARTICLE_DATE_TIME\"))            # 날짜 추가\n",
        "\n",
        "            except IndexError:\n",
        "                print(\"삭제된 기사\")\n",
        "\n",
        "        for t in title_list:\n",
        "            self.title.append(clean(t.text))\n",
        "\n",
        "        for c in content_list:\n",
        "            self.content.append(clean(c.text))\n",
        "\n",
        "        for d in date_list:\n",
        "            self.date.append(d.text)\n",
        "\n",
        "        for i in img_list:\n",
        "            self.img.append(i)\n",
        "\n",
        "    def getEntertainmentContent(self, url_list):    # 연예\n",
        "        title_list = []\n",
        "        content_list = []\n",
        "        date_list = []\n",
        "        cnt = 1\n",
        "\n",
        "        for url in url_list:\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            print(cnt)\n",
        "            cnt+=1\n",
        "\n",
        "            try:\n",
        "                title_list.extend(soup.select(\".end_tit\"))                      # 제목\n",
        "\n",
        "                c = soup.find_all(attrs={\"class\" : \"article_body\"})             # 본문\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"end_photo_org\"}):             # 이미지 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "                if c[0].find(attrs={\"class\" : \"caption\"}):                      # 이미지 설명 없애기\n",
        "                    c[0].find(attrs={\"class\" : \"caption\"}).decompose()\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"video_area\"}):                # 영상 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"video_area\"}).decompose()       # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "                while c[0].find(attrs={\"name\" : \"iframe\"}):\n",
        "                    c[0].find(attrs={\"name\" : \"iframe\"}).decompose()\n",
        "\n",
        "                content_list.extend(c)\n",
        "\n",
        "                date_list.extend(soup.select_one(\".author em\"))                 # 날짜\n",
        "\n",
        "            except IndexError:\n",
        "                print(\"삭제된 기사\")\n",
        "\n",
        "        for t in title_list:\n",
        "            self.title.append(clean(t.text))\n",
        "\n",
        "        for c in content_list:\n",
        "            self.content.append(clean(c.text))\n",
        "\n",
        "        for d in date_list:\n",
        "            self.date.append(d.text)\n",
        "\n",
        "\n",
        "    def getSportsContent(self, url_list):   # 스포츠\n",
        "        title_list = []\n",
        "        content_list = []\n",
        "        date_list = []\n",
        "        cnt = 1\n",
        "\n",
        "        for url in url_list:\n",
        "\n",
        "            browser.get(url)                                                    # 스포츠 기사만 여기서 넘어가는데 굉장히 진짜 대박 오래 걸림 왜 그럴까? 스포츠\n",
        "            \n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            print(cnt)\n",
        "            cnt+=1\n",
        "\n",
        "            title_list.extend(soup.select(\".news_headline .title\"))             # 제목\n",
        "\n",
        "            c = soup.find_all(attrs={\"class\" : \"news_end\"})                     # 본문\n",
        "\n",
        "            while c[0].find(attrs={\"class\" : \"end_photo_org\"}):                 # 이미지 있는 만큼\n",
        "                c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()        # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "            while c[0].find(attrs={\"class\" : \"image\"}):\n",
        "                c[0].find(attrs={\"class\" : \"image\"}).decompose()\n",
        "\n",
        "            while c[0].find(attrs={\"class\" : \"vod_area\"}):                      # 영상 있는 만큼\n",
        "                c[0].find(attrs={\"class\" : \"vod_area\"}).decompose()             # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "            if c[0].find(attrs={\"class\" : \"source\"}): c[0].find(attrs={\"class\" : \"source\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"byline\"}): c[0].find(attrs={\"class\" : \"byline\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"reporter_area\"}): c[0].find(attrs={\"class\" : \"reporter_area\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"copyright\"}): c[0].find(attrs={\"class\" : \"copyright\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"categorize\"}): c[0].find(attrs={\"class\" : \"categorize\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"promotion\"}): c[0].find(attrs={\"class\" : \"promotion\"}).decompose()\n",
        "\n",
        "            content_list.extend(c)\n",
        "\n",
        "            date_list.extend(soup.select_one(\".info span\"))               # 날짜\n",
        "\n",
        "        for t in title_list:\n",
        "            self.title.append(clean(t.text))\n",
        "\n",
        "        for c in content_list:\n",
        "            self.content.append(clean(c.text))\n",
        "\n",
        "        for d in date_list:\n",
        "            self.date.append(d.text)\n",
        "\n",
        "    def makeDataFrame(self, all_url, category):    # 수집한 데이터를 데이터프레임으로 변환\n",
        "\n",
        "        article_df = pd.DataFrame({\"category\" : category,\n",
        "                                   \"date\" : self.date,\n",
        "                                   \"title\" : self.title,\n",
        "                                   \"content\" : self.content,\n",
        "                                   \"img\" : self.img,\n",
        "                                   \"url\" : all_url})\n",
        "\n",
        "        return article_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s72jMBFSYAb"
      },
      "source": [
        "# 군집화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2mIzCAwI4X"
      },
      "source": [
        "**DBSCAN**\n",
        "- https://bcho.tistory.com/1205\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jnZHytpwcLa"
      },
      "outputs": [],
      "source": [
        "# 카테고리 별로 군집화\n",
        "# cluster_number 열에 군집 번호 생성\n",
        "def addClusterNumber(df, vector_list):\n",
        "    cluster_number_list = []\n",
        "\n",
        "    for vector in vector_list:\n",
        "        model = DBSCAN(eps=0.1, min_samples=1, metric='cosine')\n",
        "        result = model.fit_predict(vector)\n",
        "        cluster_number_list.extend(result)\n",
        "\n",
        "    df['cluster_number'] = cluster_number_list  # 군집 번호 칼럼 추가\n",
        "\n",
        "\n",
        "def getClusteredArticle(df): # 카테고리 별로 군집의 개수를 센다.\n",
        "    category_names = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\", \"연예\", \"스포츠\"]\n",
        "    cluster_counts_df = pd.DataFrame({'category' : [\"\"],\n",
        "                                    'cluster_number' : [0],\n",
        "                                    'cluster_count' : [0]})\n",
        "\n",
        "    for i in range(8):\n",
        "        t = df[df['category'] == category_names[i]]['cluster_number'].value_counts().reset_index()\n",
        "        t.columns = ['cluster_number', 'cluster_count']\n",
        "        t['category'] = [category_names[i]] * len(t)\n",
        "\n",
        "        cluster_counts_df = pd.concat([cluster_counts_df, t])\n",
        "\n",
        "    cluster_counts_df = cluster_counts_df[cluster_counts_df['cluster_count'] != 0]\n",
        "\n",
        "    # 상위 군집 10개씩만 추출\n",
        "    cluster_counts_df = cluster_counts_df[cluster_counts_df.index < 10]\n",
        "\n",
        "    return cluster_counts_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "import pandas as pd\n",
        "# from summa.summarizer import summarize\n",
        "\n",
        "def getSummaryArticle(article_df, cluster_counts_df):\n",
        "    summary_article = pd.DataFrame(columns=[\"category\", \"title\", \"content\", \"url\"])\n",
        "\n",
        "    for i in range(len(cluster_counts_df)):\n",
        "        category_name, cluster_number = cluster_counts_df.iloc[i, 0:2]    # 카테고리 이름, 군집 번호\n",
        "\n",
        "        temp_df = article_df[(article_df['category'] == category_name) & (article_df['cluster_number'] == cluster_number)]\n",
        "\n",
        "        category = temp_df[\"category\"].iloc[0]          # 카테고리\n",
        "        title = temp_df[\"title\"].iloc[0]                # 일단은 첫 번째 뉴스 제목\n",
        "        content = \"\".join(temp_df[\"content\"])     # 본문 내용 여러개를 하나의 문자열로 합쳐서 요약\n",
        "\n",
        "        url = \",\".join(list(temp_df[\"url\"]))                      # 전체 링크\n",
        "\n",
        "        try:\n",
        "            summary_content = summarize(content, ratio=0.3)\n",
        "            if not summary_content:     # 요약문이 비어있으면 (너무 짧아서?)\n",
        "                summary_content = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "        except:\n",
        "            summary_content = content\n",
        "        finally:\n",
        "            summary_article = summary_article.append({\n",
        "                \"category\": category,\n",
        "                \"title\": title,\n",
        "                \"content\": summary_content,\n",
        "                \"url\": url\n",
        "            }, ignore_index=True)\n",
        "\n",
        "    return summary_article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 데이터베이스 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cx_Oracle as cx\n",
        "import pandas as pd\n",
        "\n",
        "# category, title, content, url\n",
        "id = \"c##2201058\"\n",
        "pw = \"p2201058\"\n",
        "url = \"10.30.3.95:1521/orcl\"\n",
        "\n",
        "conn = cx.connect(id, pw, url)\n",
        "\n",
        "def insert(summary_article):\n",
        "\n",
        "    sql = \"\"\"insert into news(news_id, cate_id, title, content, link, views)\n",
        "             values(news_id_seq.nextval, :1, :2, :3, :4, 0)\"\"\"\n",
        "\n",
        "    cur = conn.cursor()\n",
        "    cur.executemany(sql, summary_article)\n",
        "\n",
        "    cur.close()\n",
        "    conn.commit()\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxj0vmSqmxu_"
      },
      "source": [
        "# MAIN (코드 실행 페이지)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 링크 크롤링하는 객체 생성\n",
        "url_crawler = UrlCrawling()\n",
        "\n",
        "six_url = url_crawler.getSixUrl()                          # 6개 카테고리 url\n",
        "# entertainment_url = url_crawler.getEntertainmentUrl()      # 연예 url\n",
        "# sports_url = url_crawler.getSportsUrl()                    # 스포츠 url\n",
        "# all_url = six_url + entertainment_url + sports_url        # 전체 url\n",
        "category = url_crawler.category                            # 카테고리 리스트\n",
        "\n",
        "# 본문 크롤링하는 객체 생성\n",
        "content_crawler = ContentCrawling([], [], [], [])\n",
        "\n",
        "content_crawler.getSixContent(six_url)\n",
        "# content_crawler.getEntertainmentContent(entertainment_url)\n",
        "# content_crawler.getSportsContent(sports_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_df = content_crawler.makeDataFrame(six_url, category)     # 본문 데이터프레임 생성\n",
        "article_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "정치 1 페이지\n",
            "연예 1 페이지\n",
            "스포츠 1 페이지\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15764\\276737872.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcontent_crawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mContentCrawling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcontent_crawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSixContent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msix_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mcontent_crawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetEntertainmentContent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentertainment_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcontent_crawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSportsContent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msports_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15764\\436408272.py\u001b[0m in \u001b[0;36mgetSixContent\u001b[1;34m(self, url_list)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 링크 크롤링하는 객체 생성\n",
        "url_crawler = UrlCrawling()\n",
        "\n",
        "six_url = url_crawler.getSixUrl()                          # 6개 카테고리 url\n",
        "entertainment_url = url_crawler.getEntertainmentUrl()      # 연예 url\n",
        "sports_url = url_crawler.getSportsUrl()                    # 스포츠 url\n",
        "all_url = six_url + entertainment_url + sports_url        # 전체 url\n",
        "category = url_crawler.category                            # 카테고리 리스트\n",
        "\n",
        "# 본문 크롤링하는 객체 생성\n",
        "content_crawler = ContentCrawling([], [], [], [])\n",
        "\n",
        "content_crawler.getSixContent(six_url)\n",
        "content_crawler.getEntertainmentContent(entertainment_url)\n",
        "content_crawler.getSportsContent(sports_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_df = content_crawler.makeDataFrame(all_url, category)     # 본문 데이터프레임 생성\n",
        "\n",
        "article_df = getNouns(article_df)                                 # 명사 추출\n",
        "\n",
        "vector_list = getVector(article_df)                               # 명사 벡터화\n",
        "\n",
        "addClusterNumber(article_df, vector_list)                         # 군집 번호 열 생성\n",
        "cluster_counts_df = getClusteredArticle(article_df)               # 군집 개수 카운트한 df\n",
        "\n",
        "summary_article = getSummaryArticle(article_df, cluster_counts_df)     # 요약한 기사 데이터 프레임 반환\n",
        "summary_article = convertCategory(summary_article)                     # 카테고리 이름을 번호로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터베이스에 insert\n",
        "import cx_Oracle as cx\n",
        "import pandas as pd\n",
        "\n",
        "# category, title, content, url\n",
        "id = \"c##2201058\"\n",
        "pw = \"p2201058\"\n",
        "url = \"10.30.3.95:1521/orcl\"\n",
        "\n",
        "conn = cx.connect(id, pw, url)\n",
        "\n",
        "def insert(summary_article):\n",
        "\n",
        "    sql = \"\"\"insert into news(news_id, cate_id, title, content, link, views, cre_date)\n",
        "             values(news_id_seq.nextval, :1, :2, :3, :4, 0, timestamp)\"\"\"\n",
        "\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"alter session SET time_zone='Asia/Seoul'\")\n",
        "    cur.execute(\"alter session set nls_timestamp_format='yyyy/mm/dd hh24:mi:ss'\")\n",
        "    \n",
        "    cur.executemany(sql, summary_article)\n",
        "\n",
        "    cur.close()\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "ename": "DatabaseError",
          "evalue": "ORA-00984: 열을 사용할 수 없습니다",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15464\\1329625369.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_article\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15464\\3705156066.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(summary_article)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# cur.execute(\"alter session set nls_timestamp_format='yyyy/mm/dd hh24:mi:ss'\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mDatabaseError\u001b[0m: ORA-00984: 열을 사용할 수 없습니다"
          ]
        }
      ],
      "source": [
        "insert(summary_article.values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiH75fuJQei9"
      },
      "outputs": [],
      "source": [
        "# csv로 저장\n",
        "# article_df.to_csv(\"test.csv\",index=False, encoding=\"utf-8-sig\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
