{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MWaNV4H4wdNv"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# TfidfVectorizer : 자주 나오는 단어에 높은 가중치, 모든 문서에서 자주 나오는 단어에 패널티\n",
        "# DBSCAN : 밀도 기반 클러스터링 : 점 p에서 부터 거리 e (epsilon)내에 점이 m(minPts) 개 있으면 하나의 군집으로 인식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hdoJPlf8A9K1"
      },
      "outputs": [],
      "source": [
        "# KST = timezone('Asia/Seoul')    # 서울 시간\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome(options=options)\n",
        "# browser.implicitly_wait(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPDqkOU1laIT"
      },
      "source": [
        "# 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wORd2Lf8KTiT"
      },
      "outputs": [],
      "source": [
        "# 필요없는 내용 삭제 함수\n",
        "def clean(article):\n",
        "    article = re.sub('\\w{2,4}기자','',article)\n",
        "    article = re.sub('\\w{2,4} 온라인 기자','',article)\n",
        "    article = re.sub('\\w+ 기자','',article)\n",
        "    article = re.sub('\\[.{1,15}\\]','',article)\n",
        "    article = re.sub('\\w+ 기상캐스터','',article)\n",
        "    article = re.sub('사진','',article)\n",
        "    article = re.sub('포토','',article)\n",
        "    article = re.sub('\\(.*뉴스.{0,3}\\)','', article)  # (~뉴스~) 삭제\n",
        "    article = re.sub('\\S+@[a-z.]+','',article)          # 이메일 삭제\n",
        "    article = re.sub(\"(\\s=\\s)\",\"\", article)\n",
        "\n",
        "    article = re.sub('[\\n\\t\\u200b\\xa0]','',article)\n",
        "    # article = re.sub('\\n','',article)\n",
        "    # article = re.sub('\\t','',article)\n",
        "    # article = re.sub('\\u200b','',article)\n",
        "    # article = re.sub('\\xa0','',article)\n",
        "    article = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',article)\n",
        "    # article = re.sub('([a-zA-Z])','',article)   # 영어 삭제\n",
        "    # article = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘’“”|\\(\\)\\[\\]\\<\\>`\\'…》]','',article)   # 특수문자 삭제\n",
        "\n",
        "    return article\n",
        "\n",
        "\n",
        "# 본문에서 명사 뽑아내는 함수\n",
        "def getNouns(article_df):\n",
        "    okt = Okt()\n",
        "    nouns_list = []                               # 명사 리스트\n",
        "\n",
        "    for content in article_df[\"content\"]:\n",
        "        nouns_list.append(okt.nouns(content))     # 명사 추출 (리스트 반환)\n",
        "\n",
        "    article_df[\"nouns\"] = nouns_list              # 데이터 프레임에 추가\n",
        "\n",
        "    return article_df\n",
        "\n",
        "# 명사를 벡터화 하는 함수\n",
        "def getVector(article_df):    # 카테고리 별로 벡터 생성\n",
        "    category_names = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\", \"연예\", \"스포츠\"]\n",
        "    vector_list = []\n",
        "\n",
        "    for i in range(8):\n",
        "        try:\n",
        "            text = [\" \".join(noun) for noun in article_df['nouns'][article_df['category'] == category_names[i]]]    # 명사 열을 하나의 리스트에 담는다.\n",
        "\n",
        "            tfidf_vectorizer = TfidfVectorizer(min_df = 3, ngram_range=(1, 5))\n",
        "            tfidf_vectorizer.fit(text)\n",
        "            vector = tfidf_vectorizer.transform(text).toarray()                         # vector list 반환\n",
        "            vector = np.array(vector)\n",
        "            vector_list.append(vector)\n",
        "        except:\n",
        "            print(\"크롤링 안 한 카테고리 :\", category_names[i])\n",
        "\n",
        "    return vector_list\n",
        "\n",
        "def convertCategory(article_df):    # 이름으로된 카테고리를 번호로 변환\n",
        "    category = [(\"정치\", \"100\"), (\"경제\", \"101\"), (\"사회\", \"102\"), (\"생활/문화\", \"103\"), (\"세계\", \"104\"), (\"IT/과학\", \"105\"), (\"연예\", \"106\"), (\"스포츠\", \"107\")]\n",
        "\n",
        "    for name, num in category:\n",
        "        article_df[\"category\"][article_df[\"category\"] == name] = num\n",
        "\n",
        "    return article_df\n",
        "\n",
        "def removeEnglishArticle(article_df):   # 영어 기사 삭제\n",
        "    index = article_df[article_df['nouns'].apply(len) <= 5].index\n",
        "    return article_df.drop(index, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooToJ_47sTz9"
      },
      "source": [
        "## 크롤링 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VcCupDMKM9BX"
      },
      "outputs": [],
      "source": [
        "# 기사 링크 크롤링\n",
        "class UrlCrawling:\n",
        "    def __init__(self):\n",
        "        self.category_names = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\", \"연예\", \"스포츠\"]\n",
        "        self.category = []\n",
        "\n",
        "    def getSixUrl(self):    # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "        six_url = []\n",
        "        for category in range(6):     # 6\n",
        "            a_list = []\n",
        "            for page in range(1, 6):  # 1, 6\n",
        "                url = f'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={100 + category}#&date=%2000:00:00&page={page}'\n",
        "                browser.get(url)\n",
        "\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "                a_list.extend(soup.select(\".type06_headline dt+dt a\"))\n",
        "                a_list.extend(soup.select(\".type06 dt+dt a\"))\n",
        "\n",
        "                print(f\"{self.category_names[category]} {page} 페이지\")\n",
        "\n",
        "            for a in a_list:\n",
        "                six_url.append(a[\"href\"])\n",
        "                self.category.append(self.category_names[category])\n",
        "\n",
        "        return six_url\n",
        "\n",
        "\n",
        "    def getEntertainmentUrl(self):   # 연예\n",
        "        # today = str(datetime.datetime.now(KST))[:11]  # 서울 기준 시간\n",
        "        entertainment_url = []\n",
        "        a_list = []\n",
        "        today = datetime.date.today()\n",
        "\n",
        "        for page in range(1, 2):  # 1, 5\n",
        "            url = f'https://entertain.naver.com/now#sid=106&date={today}&page={page}'\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            a_list.extend(soup.select(\".news_lst li>a\"))\n",
        "\n",
        "\n",
        "            print(f\"연예 {page} 페이지\")\n",
        "\n",
        "        for a in a_list:\n",
        "            entertainment_url.append(\"https://entertain.naver.com\" + a[\"href\"])\n",
        "            self.category.append(\"연예\")\n",
        "\n",
        "        return entertainment_url\n",
        "\n",
        "    def getSportsUrl(self):    # 스포츠  (페이지마다 개수가 달라서 6페이지를 이동)\n",
        "        # today = str(datetime.datetime.now(KST))[:11].replace('-', '')  # 서울 기준 시간\n",
        "        sports_url = []\n",
        "        a_list = []\n",
        "        today = str(datetime.date.today()).replace('-', '')\n",
        "\n",
        "        for page in range(1, 2):  # 1, 7\n",
        "            url = f'https://sports.news.naver.com/general/news/index?isphoto=N&type=latest&date={today}&page={page}'\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "            a_list.extend(soup.select(\".news_list li>a\"))\n",
        "\n",
        "            print(f\"스포츠 {page} 페이지\")\n",
        "\n",
        "        for i in range(len(a_list)):\n",
        "            if i == 100:  # 100개 링크 추가했으면 멈추기\n",
        "                break\n",
        "            sports_url.append(\"https://sports.news.naver.com/news\" + re.search('\\?.+', a_list[i][\"href\"]).group())\n",
        "            self.category.append(\"스포츠\")\n",
        "\n",
        "        return sports_url\n",
        "\n",
        "\n",
        "\n",
        "# 기사 본문 크롤링\n",
        "class ContentCrawling:\n",
        "    def __init__(self, title, content, date, img):\n",
        "        self.title = title\n",
        "        self.content = content\n",
        "        self.date = date\n",
        "        self.img = img\n",
        "\n",
        "    def getSixContent(self, url_list):  # 정치, 경제, 사회, 생활/문화, 세계, IT/과학\n",
        "        title_list = []\n",
        "        content_list = []\n",
        "        date_list = []\n",
        "        img_list = []\n",
        "        cnt = 1\n",
        "\n",
        "        for url in url_list:\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            print(cnt)\n",
        "            cnt+=1\n",
        "\n",
        "            try:\n",
        "                title_list.extend(soup.select(\"#title_area span\"))              # 제목 추가\n",
        "\n",
        "                c = soup.find_all(attrs={\"id\" : \"dic_area\"})                    # 본문 가져오기\n",
        "\n",
        "                img_tag = soup.select(\".end_photo_org img\")                     # 이미지 가져오기\n",
        "\n",
        "                if img_tag:                                                     # 이미지 있으면 이미지 주소만 추출해서 리스트로 만든다.\n",
        "                    img_src_list = []\n",
        "                    for img in img_tag:\n",
        "                        img_src_list.append(img['src'])\n",
        "                    img_list.append(\",\".join(img_src_list))\n",
        "                else:\n",
        "                    img_list.append(\"\")\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"end_photo_org\"}):             # 이미지 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"vod_player_wrap\"}):           # 영상 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"vod_player_wrap\"}).decompose()  # 본문 영상에 있는 글자 없애기\n",
        "\n",
        "                if c[0].find(attrs={\"class\" : \"artical-btm\"}):                  # 하단에 제보하기 칸 있으면 삭제\n",
        "                    c[0].find(attrs={\"class\" : \"artical-btm\"}).decompose()\n",
        "\n",
        "                content_list.extend(c)                                          # 본문 추가\n",
        "\n",
        "                date_list.extend(soup.select(\"._ARTICLE_DATE_TIME\"))            # 날짜 추가\n",
        "\n",
        "            except IndexError:\n",
        "                print(\"삭제된 기사\")\n",
        "\n",
        "        for t in title_list:\n",
        "            self.title.append(clean(t.text))\n",
        "\n",
        "        for c in content_list:\n",
        "            self.content.append(clean(c.text))\n",
        "\n",
        "        for d in date_list:\n",
        "            self.date.append(d.text)\n",
        "\n",
        "        for i in img_list:\n",
        "            self.img.append(i)\n",
        "\n",
        "    def getEntertainmentContent(self, url_list):    # 연예\n",
        "        title_list = []\n",
        "        content_list = []\n",
        "        date_list = []\n",
        "        img_list = []\n",
        "        cnt = 1\n",
        "\n",
        "        for url in url_list:\n",
        "            browser.get(url)\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            print(cnt)\n",
        "            cnt+=1\n",
        "\n",
        "            try:\n",
        "                title_list.extend(soup.select(\".end_tit\"))                      # 제목 추가\n",
        "\n",
        "                c = soup.find_all(attrs={\"class\" : \"article_body\"})             # 본문 가져오기\n",
        "\n",
        "                img_tag = soup.select(\".end_photo_org img\")                     # 이미지 가져오기\n",
        "\n",
        "                if img_tag:                                                     # 이미지 있으면 이미지 주소만 추출해서 리스트로 만든다.\n",
        "                    img_src_list = []\n",
        "                    for img in img_tag:\n",
        "                        img_src_list.append(img['src'])\n",
        "                    img_list.append(\",\".join(img_src_list))\n",
        "                else:\n",
        "                    img_list.append(\"\")\n",
        "\n",
        "\n",
        "                while c[0].find(attrs={\"class\" : \"end_photo_org\"}):             # 이미지 있는 만큼\n",
        "                    c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()    # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "                if c[0].find(attrs={\"class\" : \"caption\"}):                      # 이미지 설명 없애기\n",
        "                    c[0].find(attrs={\"class\" : \"caption\"}).decompose()\n",
        "\n",
        "                while c[0].find(attrs={\"id\" : \"video_area\"}):                # 영상 있는 만큼\n",
        "                    c[0].find(attrs={\"id\" : \"video_area\"}).decompose()       # 본문 영상 없애기\n",
        "\n",
        "                while c[0].find(attrs={\"name\" : \"iframe\"}):\n",
        "                    c[0].find(attrs={\"name\" : \"iframe\"}).decompose()\n",
        "\n",
        "                content_list.extend(c)                                          # 본문 추가\n",
        "\n",
        "                date_list.extend(soup.select_one(\".author em\"))                 # 날짜 추가\n",
        "\n",
        "            except IndexError:\n",
        "                print(\"삭제된 기사\")\n",
        "\n",
        "        for t in title_list:\n",
        "            self.title.append(clean(t.text))\n",
        "\n",
        "        for c in content_list:\n",
        "            self.content.append(clean(c.text))\n",
        "\n",
        "        for d in date_list:\n",
        "            self.date.append(d.text)\n",
        "            \n",
        "        for i in img_list:\n",
        "            self.img.append(i)\n",
        "\n",
        "    def getSportsContent(self, url_list):   # 스포츠\n",
        "        title_list = []\n",
        "        content_list = []\n",
        "        date_list = []\n",
        "        img_list = []\n",
        "        cnt = 1\n",
        "\n",
        "        for url in url_list:\n",
        "\n",
        "            browser.get(url)                                                    \n",
        "            \n",
        "            time.sleep(0.5)\n",
        "\n",
        "            soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "\n",
        "            print(cnt)\n",
        "            cnt+=1\n",
        "\n",
        "            title_list.extend(soup.select(\".news_headline .title\"))             # 제목 추가 \n",
        "\n",
        "            c = soup.find_all(attrs={\"class\" : \"news_end\"})                     # 본문 가져오기\n",
        "\n",
        "            img_tag = soup.select(\".end_photo_org img\")                     # 이미지 가져오기\n",
        "\n",
        "            if img_tag:                                                     # 이미지 있으면 이미지 주소만 추출해서 리스트로 만든다.\n",
        "                img_src_list = []\n",
        "                for img in img_tag:\n",
        "                    img_src_list.append(img['src'])\n",
        "                img_list.append(\",\".join(img_src_list))\n",
        "            else:\n",
        "                img_list.append(\"\")\n",
        "\n",
        "            while c[0].find(attrs={\"class\" : \"end_photo_org\"}):                 # 이미지 있는 만큼\n",
        "                c[0].find(attrs={\"class\" : \"end_photo_org\"}).decompose()        # 본문 이미지에 있는 글자 없애기\n",
        "\n",
        "            while c[0].find(attrs={\"class\" : \"image\"}):\n",
        "                c[0].find(attrs={\"class\" : \"image\"}).decompose()\n",
        "\n",
        "            while c[0].find(attrs={\"class\" : \"vod_area\"}):                      # 영상 있는 만큼\n",
        "                c[0].find(attrs={\"class\" : \"vod_area\"}).decompose()             # 본문 영상 없애기\n",
        "\n",
        "            if c[0].find(attrs={\"class\" : \"source\"}): c[0].find(attrs={\"class\" : \"source\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"byline\"}): c[0].find(attrs={\"class\" : \"byline\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"reporter_area\"}): c[0].find(attrs={\"class\" : \"reporter_area\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"copyright\"}): c[0].find(attrs={\"class\" : \"copyright\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"categorize\"}): c[0].find(attrs={\"class\" : \"categorize\"}).decompose()\n",
        "            if c[0].find(attrs={\"class\" : \"promotion\"}): c[0].find(attrs={\"class\" : \"promotion\"}).decompose()\n",
        "\n",
        "            content_list.extend(c)                                        # 본문 추가\n",
        "\n",
        "            date_list.extend(soup.select_one(\".info span\"))               # 날짜 추가\n",
        "\n",
        "        for t in title_list:\n",
        "            self.title.append(clean(t.text))\n",
        "\n",
        "        for c in content_list:\n",
        "            self.content.append(clean(c.text))\n",
        "\n",
        "        for d in date_list:\n",
        "            d = (d.text)[5:]\n",
        "            self.date.append(d)\n",
        "\n",
        "        for i in img_list:\n",
        "            self.img.append(i)\n",
        "\n",
        "    def makeDataFrame(self, all_url, category):    # 수집한 데이터를 데이터프레임으로 변환\n",
        "\n",
        "        article_df = pd.DataFrame({\"category\" : category,\n",
        "                                   \"date\" : self.date,\n",
        "                                   \"title\" : self.title,\n",
        "                                   \"content\" : self.content,\n",
        "                                   \"img\" : self.img,\n",
        "                                   \"url\" : all_url})\n",
        "\n",
        "        return article_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s72jMBFSYAb"
      },
      "source": [
        "# 군집화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2mIzCAwI4X"
      },
      "source": [
        "**DBSCAN**\n",
        "- https://bcho.tistory.com/1205\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jnZHytpwcLa"
      },
      "outputs": [],
      "source": [
        "# 카테고리 별로 군집화\n",
        "# cluster_number 열에 군집 번호 생성\n",
        "def addClusterNumber(df, vector_list):\n",
        "    cluster_number_list = []\n",
        "\n",
        "    for vector in vector_list:\n",
        "        model = DBSCAN(eps=0.1, min_samples=1, metric='cosine')\n",
        "        result = model.fit_predict(vector)\n",
        "        cluster_number_list.extend(result)\n",
        "\n",
        "    df['cluster_number'] = cluster_number_list  # 군집 번호 칼럼 추가\n",
        "\n",
        "\n",
        "def getClusteredArticle(df): # 카테고리 별로 군집의 개수를 센다.\n",
        "    category_names = [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\", \"연예\", \"스포츠\"]\n",
        "    cluster_counts_df = pd.DataFrame({'category' : [\"\"],\n",
        "                                    'cluster_number' : [0],\n",
        "                                    'cluster_count' : [0]})\n",
        "\n",
        "    for i in range(8):\n",
        "        t = df[df['category'] == category_names[i]]['cluster_number'].value_counts().reset_index()\n",
        "        t.columns = ['cluster_number', 'cluster_count']\n",
        "        t['category'] = [category_names[i]] * len(t)\n",
        "\n",
        "        cluster_counts_df = pd.concat([cluster_counts_df, t])\n",
        "\n",
        "    cluster_counts_df = cluster_counts_df[cluster_counts_df['cluster_count'] != 0]\n",
        "\n",
        "    # 상위 군집 10개씩만 추출\n",
        "    cluster_counts_df = cluster_counts_df[cluster_counts_df.index < 10]\n",
        "\n",
        "    return cluster_counts_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "import pandas as pd\n",
        "# from summa.summarizer import summarize\n",
        "\n",
        "def getSummaryArticle(article_df, cluster_counts_df):\n",
        "    summary_article = pd.DataFrame(columns=[\"category\", \"title\", \"content\", \"img\", \"url\"])\n",
        "\n",
        "    for i in range(len(cluster_counts_df)):\n",
        "        category_name, cluster_number = cluster_counts_df.iloc[i, 0:2]    # 카테고리 이름, 군집 번호\n",
        "\n",
        "        temp_df = article_df[(article_df['category'] == category_name) & (article_df['cluster_number'] == cluster_number)]\n",
        "\n",
        "        category = temp_df[\"category\"].iloc[0]          # 카테고리\n",
        "        title = temp_df[\"title\"].iloc[0]                # 일단은 첫 번째 뉴스 제목\n",
        "        content = \"\".join(temp_df[\"content\"])           # 본문 내용 여러개를 하나의 문자열로 합쳐서 요약\n",
        "        img = \",\".join(list(temp_df[\"img\"]))            # 전체 이미지\n",
        "        url = \",\".join(list(temp_df[\"url\"]))            # 전체 링크\n",
        "\n",
        "        try:\n",
        "            summary_content = summarize(content, ratio=0.2)\n",
        "            if not summary_content:     # 요약문이 비어있으면 (너무 짧아서?)\n",
        "                summary_content = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "        except:\n",
        "            summary_content = content\n",
        "        finally:\n",
        "            summary_article = summary_article.append({\n",
        "                \"category\": category,\n",
        "                \"title\": title,\n",
        "                \"content\": summary_content,\n",
        "                \"img\": img,\n",
        "                \"url\": url\n",
        "            }, ignore_index=True)\n",
        "\n",
        "    return summary_article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 데이터베이스 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cx_Oracle as cx\n",
        "import pandas as pd\n",
        "\n",
        "# category, title, content, url\n",
        "id = \"c##2201058\"\n",
        "pw = \"p2201058\"\n",
        "url = \"10.30.3.95:1521/orcl\"\n",
        "\n",
        "conn = cx.connect(id, pw, url)\n",
        "\n",
        "def insert(summary_article):\n",
        "\n",
        "    sql = \"\"\"insert into news(news_id, cate_id, title, content, img, link, views)\n",
        "             values(news_id_seq.nextval, :1, :2, :3, :4, :5, 0)\"\"\"\n",
        "\n",
        "    cur = conn.cursor()\n",
        "    cur.executemany(sql, summary_article)\n",
        "\n",
        "    cur.close()\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def select():\n",
        "\n",
        "    sql = \"\"\"select * from news\n",
        "             order by to_number(news_id)\"\"\"\n",
        "    \n",
        "    cur = conn.cursor()\n",
        "    cur.execute(sql)\n",
        "\n",
        "    df = pd.read_sql(sql, con = conn)\n",
        "    df[\"CONTENT\"] = df[\"CONTENT\"].astype(\"string\")      # CLOB 데이터 타입을 string로 변경해야 df로 가져올 수 있음\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxj0vmSqmxu_"
      },
      "source": [
        "# MAIN (코드 실행 페이지)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 링크 크롤링하는 객체 생성\n",
        "url_crawler = UrlCrawling()\n",
        "\n",
        "six_url = url_crawler.getSixUrl()                          # 6개 카테고리 url\n",
        "entertainment_url = url_crawler.getEntertainmentUrl()      # 연예 url\n",
        "sports_url = url_crawler.getSportsUrl()                    # 스포츠 url\n",
        "all_url = six_url + entertainment_url + sports_url        # 전체 url\n",
        "category = url_crawler.category                            # 카테고리 리스트\n",
        "\n",
        "# 본문 크롤링하는 객체 생성\n",
        "content_crawler = ContentCrawling([], [], [], [])\n",
        "\n",
        "content_crawler.getSixContent(six_url)\n",
        "content_crawler.getEntertainmentContent(entertainment_url)\n",
        "content_crawler.getSportsContent(sports_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'all_url' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10188\\2005628380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marticle_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent_crawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakeDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# 본문 데이터프레임 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0marticle_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_df\u001b[0m\u001b[1;33m)\u001b[0m                                 \u001b[1;31m# 명사 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvector_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_df\u001b[0m\u001b[1;33m)\u001b[0m                               \u001b[1;31m# 명사 벡터화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'all_url' is not defined"
          ]
        }
      ],
      "source": [
        "article_df = content_crawler.makeDataFrame(all_url, category)     # 본문 데이터프레임 생성\n",
        "\n",
        "article_df = getNouns(article_df)                                 # 명사 추출\n",
        "\n",
        "vector_list = getVector(article_df)                               # 명사 벡터화\n",
        "\n",
        "addClusterNumber(article_df, vector_list)                         # 군집 번호 열 생성\n",
        "cluster_counts_df = getClusteredArticle(article_df)               # 군집 개수 카운트한 df\n",
        "\n",
        "summary_article = getSummaryArticle(article_df, cluster_counts_df)     # 요약한 기사 데이터 프레임 반환\n",
        "summary_article = convertCategory(summary_article)                     # 카테고리 이름을 번호로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NEWS_ID</th>\n",
              "      <th>CATE_ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>IMG</th>\n",
              "      <th>LINK</th>\n",
              "      <th>VIEWS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>北, 9·19 합의로 파괴한 GP에 병력·중화기 투입…경계초소 설치\"</td>\n",
              "      <td>군 관계자 \"파괴·철수한 11개 GP서 유사한 상황…해안포 개방도 많이 늘어\"북한군...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/087/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/087/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>'서울 험지' 출마한다던 하태경, 같은 당 최재형 지역구 종로로</td>\n",
              "      <td>말린다고 되겠나\"부산 지역구를 버리고 차기 총선에 서울 험지 출마를 선언한 국민의힘...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/002/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/002/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>군 “북 위성체 궤도진입 성공… 위성기술은 아직”</td>\n",
              "      <td>북 '한반도·하와이' 촬영 주장엔 \"보여주기식 선전\"북한이 최근 발사한 군사정찰위성...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/277/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/277/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>극단적 정쟁에 갇힌 정치, 사람 교체가 답</td>\n",
              "      <td>그동안 장 의원을 비롯한 국민의힘 영남권 중진 의원들은 인요한 혁신위원장으로부터 '...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/586/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/586/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>100</td>\n",
              "      <td>민주, '권리당원 비중 강화' 당무위 의결…비명계 반발</td>\n",
              "      <td>권칠승 수석대변인은 이날 오전 국회에서 열린 당무위 들과 만나 전당대회 후보자 본선...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/277/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/277/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>100</td>\n",
              "      <td>국정원장, 조직통솔·기강확립·방첩강화 차원서 '외부인사' 무게</td>\n",
              "      <td>천영우·김용현·권영세 등 거론…대통령실 \"늦어도 한 달 내 지명\"내부 발탁 가능성도...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/001/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/001/001...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>100</td>\n",
              "      <td>'김기현 신경전'에 '집안 싸움'까지…與 혁신위 운명은?</td>\n",
              "      <td>사퇴를 표명했던 외부 영입 위원들과 정치인 출신 위원들의 생각이 여전히 다른 가운데...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/586/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/586/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>100</td>\n",
              "      <td>정권교체때마다 불거진 ‘국정원 권력투쟁’ 뿌리뽑는다</td>\n",
              "      <td>■ 윤 대통령, 국정원 전면쇄신 방침권력다툼 인사는 전면배제키로진흙탕 파벌대립 위험...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/021/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/021/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>100</td>\n",
              "      <td>北 '9·19합의 파기' 선언 뒤 DMZ 내 초소 복구·중화기 배치</td>\n",
              "      <td>軍 \"기존 시설물 복원 나선 듯… 서해 해안포 개방도 늘어\" 박응진  = 북한군이 ...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/421/2023/11/...</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/421/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "      <td>한동훈·이정재, 함께 식사...SNS 올라온 '인증샷' 화제</td>\n",
              "      <td>배우 이정재 씨 만나서 함께 식사한 장면을 시민들이 을 찍어서 SNS에 올리면서 화...</td>\n",
              "      <td>None</td>\n",
              "      <td>https://n.news.naver.com/mnews/article/052/000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>106</td>\n",
              "      <td>은우, 살뜰한 형아…'승무원 출신 아내♥' 김준호 \"동생 하나 더 낳을까?\"('슈돌')</td>\n",
              "      <td>나아가 은우는 아빠 김준호가 묻는 말에 손을 번쩍 들으며, 행동으로 정답을 보여주는...</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/312/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=312&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>106</td>\n",
              "      <td>이성민, 처절한 복수전 돌입…\"똑같이 갚아줄게\" (운수 오진 날)</td>\n",
              "      <td>'운수 오진 날' 이성민의 처절한 복수가 시작된다.티빙 오리지널 시리즈 '운수 오진...</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/311/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=311&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>106</td>\n",
              "      <td>女스타 \"출산, 안 아팠다\"...\"애 낳는거 쉬워 보이게 만든다\" 비판↑</td>\n",
              "      <td>두 사람은 슬하에 아들 하나 딸 하나를 두고 있으며, 두 아이 모두 제왕절개를 통해...</td>\n",
              "      <td>https://mimgnews.pstatic.net/image/213/2023/11...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=213&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>106</td>\n",
              "      <td>둘 다 음성인데…출국금지, 지드래곤 해제-이선균 연장</td>\n",
              "      <td>그러나 결백을 명확하게 주장하고 있는 권씨와 달리 이씨는 마약을 건네 준 것으로 알...</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/003/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=003&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>106</td>\n",
              "      <td>'아쿠아맨2', '노량'과 정면승부…12월 20일 개봉 확정</td>\n",
              "      <td>영화 '아쿠아맨과 로스트 킹덤'(이하 '아쿠아맨2')이 '노량: 죽음의 바다'로 정...</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/416/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=416&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>106</td>\n",
              "      <td>BLACKPINK JENNIE Putting on Airs of Elegance</td>\n",
              "      <td>(MHNSPORTS Hyeonha Jeong reporter)BLACKPINK JE...</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/445/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=445&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>106</td>\n",
              "      <td>Kim Youjung Having with an Lovable Smile</td>\n",
              "      <td>(MHNSPORTS Hyeonha Jeong reporter) The product...</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/445/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=445&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>106</td>\n",
              "      <td>\"한번 꼬셔볼까?\"…더 화끈해진 '솔로지옥3' 출연진 공개</td>\n",
              "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/415/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=415&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>106</td>\n",
              "      <td>'마에스트라' 감독 \"이영애? 매 순간 소름...이 세상 모든 점수 다 드리고 싶어\"</td>\n",
              "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td>\n",
              "      <td>https://ssl.pstatic.net/mimgnews/image/445/202...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=445&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>106</td>\n",
              "      <td>'대학생 버스킹 성지' 유버스킹, 이제 방송으로 만난다</td>\n",
              "      <td>OBS 음악방송 '유버스킹 청춘레코드'(MHN스포츠 ) 대학생들의 버스킹 축제'유버...</td>\n",
              "      <td>https://mimgnews.pstatic.net/image/445/2023/11...</td>\n",
              "      <td>https://entertain.naver.com/now/read?oid=445&amp;a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>107</td>\n",
              "      <td>‘스키 여제’ 시프린, 월드컵 최초 90승…100승도 넘본다</td>\n",
              "      <td>현역 최고의 기량을 뽐내고 있는 시프린은 올 시즌 전인미답의 100승 달성을 향해 ...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/005/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=005&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>107</td>\n",
              "      <td>'맏형' 원성진, 농심배 한국 첫승 도전…30일 셰얼하오와 격돌</td>\n",
              "      <td>셰얼하오는 이번 농심배 1라운드에서도 파죽의 3연승을 거둔 상태다.\n",
              "한국은 최근 3...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/001/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=001&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>107</td>\n",
              "      <td>경륜 28기 데뷔 한 달 앞으로! 눈여겨봐야 할 선수는?</td>\n",
              "      <td>국가대표 출신으로 27살이라는 나이에도 불구하고 탄탄한 기본기를 갖춰 ;포스트 임채...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/117/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=117&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>107</td>\n",
              "      <td>국내 최초 바둑 전 연다</td>\n",
              "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td>\n",
              "      <td>None</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=370&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>107</td>\n",
              "      <td>“역사를 만들었다, 내 기록 깨지길… 먼 훗날에”</td>\n",
              "      <td>■ 조코비치, 테니스 사상 첫 ‘세계 1위 400주’ 돌파연말 ‘세계랭킹 1위’ 8...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/021/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=021&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>107</td>\n",
              "      <td>국립스포츠박물관 건립 첫 삽…장미란 차관 \"스포츠 유산 전달 거점\"</td>\n",
              "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</td>\n",
              "      <td>https://imgnews.pstatic.net/image/003/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=003&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>107</td>\n",
              "      <td>'AG 접영 챔프' 백인철, 생애 두 번째 세계선수권대회 자력 진출</td>\n",
              "      <td>(MHN스포츠 ) 수영 국가대표 백인철(부산광역시중구청)이 생애 두 번째 세계수영선...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/445/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=445&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>107</td>\n",
              "      <td>강원동계청소년올림픽 조직위, '알리바바클라우드 해커톤' 접수 시</td>\n",
              "      <td>2024 강원 동계청소년올림픽대회 조직위원회는 알리바바클라우드와 함께 '2024...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/425/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=425&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>107</td>\n",
              "      <td>'캐나다 나와' 韓 남자 테니스, 사상 첫 3년 연속 국가 대항전 본선 도전</td>\n",
              "      <td>올해 본선에 진출한 16개국 중 우승팀 이탈리아, 준우승팀 호주와 와일드카드를 받은...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/079/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=079&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>107</td>\n",
              "      <td>'세컨드서브까지 8초 제한' 한층 빨라진 넥젠 파이널스</td>\n",
              "      <td>6게임 1세트 방식이 아닌 4게임 1세트 방식으로 5세트 경기를 진행하고 듀스를 없...</td>\n",
              "      <td>https://imgnews.pstatic.net/image/481/2023/11/...</td>\n",
              "      <td>https://sports.news.naver.com/news?oid=481&amp;aid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   NEWS_ID CATE_ID                                             TITLE  \\\n",
              "0        1     100            北, 9·19 합의로 파괴한 GP에 병력·중화기 투입…경계초소 설치\"   \n",
              "1        2     100               '서울 험지' 출마한다던 하태경, 같은 당 최재형 지역구 종로로   \n",
              "2        3     100                       군 “북 위성체 궤도진입 성공… 위성기술은 아직”   \n",
              "3        4     100                           극단적 정쟁에 갇힌 정치, 사람 교체가 답   \n",
              "4        5     100                    민주, '권리당원 비중 강화' 당무위 의결…비명계 반발   \n",
              "5        6     100                국정원장, 조직통솔·기강확립·방첩강화 차원서 '외부인사' 무게   \n",
              "6        7     100                   '김기현 신경전'에 '집안 싸움'까지…與 혁신위 운명은?   \n",
              "7        8     100                      정권교체때마다 불거진 ‘국정원 권력투쟁’ 뿌리뽑는다   \n",
              "8        9     100             北 '9·19합의 파기' 선언 뒤 DMZ 내 초소 복구·중화기 배치   \n",
              "9       10     100               한동훈·이정재, 함께 식사...SNS 올라온 '인증샷' 화제     \n",
              "10      11     106  은우, 살뜰한 형아…'승무원 출신 아내♥' 김준호 \"동생 하나 더 낳을까?\"('슈돌')   \n",
              "11      12     106              이성민, 처절한 복수전 돌입…\"똑같이 갚아줄게\" (운수 오진 날)   \n",
              "12      13     106          女스타 \"출산, 안 아팠다\"...\"애 낳는거 쉬워 보이게 만든다\" 비판↑   \n",
              "13      14     106                     둘 다 음성인데…출국금지, 지드래곤 해제-이선균 연장   \n",
              "14      15     106                 '아쿠아맨2', '노량'과 정면승부…12월 20일 개봉 확정   \n",
              "15      16     106     BLACKPINK JENNIE Putting on Airs of Elegance    \n",
              "16      17     106         Kim Youjung Having with an Lovable Smile    \n",
              "17      18     106                  \"한번 꼬셔볼까?\"…더 화끈해진 '솔로지옥3' 출연진 공개   \n",
              "18      19     106   '마에스트라' 감독 \"이영애? 매 순간 소름...이 세상 모든 점수 다 드리고 싶어\"   \n",
              "19      20     106                    '대학생 버스킹 성지' 유버스킹, 이제 방송으로 만난다   \n",
              "20      21     107                 ‘스키 여제’ 시프린, 월드컵 최초 90승…100승도 넘본다   \n",
              "21      22     107               '맏형' 원성진, 농심배 한국 첫승 도전…30일 셰얼하오와 격돌   \n",
              "22      23     107                   경륜 28기 데뷔 한 달 앞으로! 눈여겨봐야 할 선수는?   \n",
              "23      24     107                                     국내 최초 바둑 전 연다   \n",
              "24      25     107                       “역사를 만들었다, 내 기록 깨지길… 먼 훗날에”   \n",
              "25      26     107             국립스포츠박물관 건립 첫 삽…장미란 차관 \"스포츠 유산 전달 거점\"   \n",
              "26      27     107             'AG 접영 챔프' 백인철, 생애 두 번째 세계선수권대회 자력 진출   \n",
              "27      28     107               강원동계청소년올림픽 조직위, '알리바바클라우드 해커톤' 접수 시   \n",
              "28      29     107        '캐나다 나와' 韓 남자 테니스, 사상 첫 3년 연속 국가 대항전 본선 도전   \n",
              "29      30     107                    '세컨드서브까지 8초 제한' 한층 빨라진 넥젠 파이널스   \n",
              "\n",
              "                                              CONTENT  \\\n",
              "0   군 관계자 \"파괴·철수한 11개 GP서 유사한 상황…해안포 개방도 많이 늘어\"북한군...   \n",
              "1   말린다고 되겠나\"부산 지역구를 버리고 차기 총선에 서울 험지 출마를 선언한 국민의힘...   \n",
              "2   북 '한반도·하와이' 촬영 주장엔 \"보여주기식 선전\"북한이 최근 발사한 군사정찰위성...   \n",
              "3   그동안 장 의원을 비롯한 국민의힘 영남권 중진 의원들은 인요한 혁신위원장으로부터 '...   \n",
              "4   권칠승 수석대변인은 이날 오전 국회에서 열린 당무위 들과 만나 전당대회 후보자 본선...   \n",
              "5   천영우·김용현·권영세 등 거론…대통령실 \"늦어도 한 달 내 지명\"내부 발탁 가능성도...   \n",
              "6   사퇴를 표명했던 외부 영입 위원들과 정치인 출신 위원들의 생각이 여전히 다른 가운데...   \n",
              "7   ■ 윤 대통령, 국정원 전면쇄신 방침권력다툼 인사는 전면배제키로진흙탕 파벌대립 위험...   \n",
              "8   軍 \"기존 시설물 복원 나선 듯… 서해 해안포 개방도 늘어\" 박응진  = 북한군이 ...   \n",
              "9   배우 이정재 씨 만나서 함께 식사한 장면을 시민들이 을 찍어서 SNS에 올리면서 화...   \n",
              "10  나아가 은우는 아빠 김준호가 묻는 말에 손을 번쩍 들으며, 행동으로 정답을 보여주는...   \n",
              "11  '운수 오진 날' 이성민의 처절한 복수가 시작된다.티빙 오리지널 시리즈 '운수 오진...   \n",
              "12  두 사람은 슬하에 아들 하나 딸 하나를 두고 있으며, 두 아이 모두 제왕절개를 통해...   \n",
              "13  그러나 결백을 명확하게 주장하고 있는 권씨와 달리 이씨는 마약을 건네 준 것으로 알...   \n",
              "14  영화 '아쿠아맨과 로스트 킹덤'(이하 '아쿠아맨2')이 '노량: 죽음의 바다'로 정...   \n",
              "15  (MHNSPORTS Hyeonha Jeong reporter)BLACKPINK JE...   \n",
              "16  (MHNSPORTS Hyeonha Jeong reporter) The product...   \n",
              "17     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   \n",
              "18     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   \n",
              "19  OBS 음악방송 '유버스킹 청춘레코드'(MHN스포츠 ) 대학생들의 버스킹 축제'유버...   \n",
              "20  현역 최고의 기량을 뽐내고 있는 시프린은 올 시즌 전인미답의 100승 달성을 향해 ...   \n",
              "21  셰얼하오는 이번 농심배 1라운드에서도 파죽의 3연승을 거둔 상태다.\n",
              "한국은 최근 3...   \n",
              "22  국가대표 출신으로 27살이라는 나이에도 불구하고 탄탄한 기본기를 갖춰 ;포스트 임채...   \n",
              "23     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   \n",
              "24  ■ 조코비치, 테니스 사상 첫 ‘세계 1위 400주’ 돌파연말 ‘세계랭킹 1위’ 8...   \n",
              "25     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   \n",
              "26  (MHN스포츠 ) 수영 국가대표 백인철(부산광역시중구청)이 생애 두 번째 세계수영선...   \n",
              "27    2024 강원 동계청소년올림픽대회 조직위원회는 알리바바클라우드와 함께 '2024...   \n",
              "28  올해 본선에 진출한 16개국 중 우승팀 이탈리아, 준우승팀 호주와 와일드카드를 받은...   \n",
              "29  6게임 1세트 방식이 아닌 4게임 1세트 방식으로 5세트 경기를 진행하고 듀스를 없...   \n",
              "\n",
              "                                                  IMG  \\\n",
              "0   https://imgnews.pstatic.net/image/087/2023/11/...   \n",
              "1   https://imgnews.pstatic.net/image/002/2023/11/...   \n",
              "2   https://imgnews.pstatic.net/image/277/2023/11/...   \n",
              "3   https://imgnews.pstatic.net/image/586/2023/11/...   \n",
              "4   https://imgnews.pstatic.net/image/277/2023/11/...   \n",
              "5   https://imgnews.pstatic.net/image/001/2023/11/...   \n",
              "6   https://imgnews.pstatic.net/image/586/2023/11/...   \n",
              "7   https://imgnews.pstatic.net/image/021/2023/11/...   \n",
              "8   https://imgnews.pstatic.net/image/421/2023/11/...   \n",
              "9                                                None   \n",
              "10  https://ssl.pstatic.net/mimgnews/image/312/202...   \n",
              "11  https://ssl.pstatic.net/mimgnews/image/311/202...   \n",
              "12  https://mimgnews.pstatic.net/image/213/2023/11...   \n",
              "13  https://ssl.pstatic.net/mimgnews/image/003/202...   \n",
              "14  https://ssl.pstatic.net/mimgnews/image/416/202...   \n",
              "15  https://ssl.pstatic.net/mimgnews/image/445/202...   \n",
              "16  https://ssl.pstatic.net/mimgnews/image/445/202...   \n",
              "17  https://ssl.pstatic.net/mimgnews/image/415/202...   \n",
              "18  https://ssl.pstatic.net/mimgnews/image/445/202...   \n",
              "19  https://mimgnews.pstatic.net/image/445/2023/11...   \n",
              "20  https://imgnews.pstatic.net/image/005/2023/11/...   \n",
              "21  https://imgnews.pstatic.net/image/001/2023/11/...   \n",
              "22  https://imgnews.pstatic.net/image/117/2023/11/...   \n",
              "23                                               None   \n",
              "24  https://imgnews.pstatic.net/image/021/2023/11/...   \n",
              "25  https://imgnews.pstatic.net/image/003/2023/11/...   \n",
              "26  https://imgnews.pstatic.net/image/445/2023/11/...   \n",
              "27  https://imgnews.pstatic.net/image/425/2023/11/...   \n",
              "28  https://imgnews.pstatic.net/image/079/2023/11/...   \n",
              "29  https://imgnews.pstatic.net/image/481/2023/11/...   \n",
              "\n",
              "                                                 LINK  VIEWS  \n",
              "0   https://n.news.naver.com/mnews/article/087/000...      0  \n",
              "1   https://n.news.naver.com/mnews/article/002/000...      0  \n",
              "2   https://n.news.naver.com/mnews/article/277/000...      0  \n",
              "3   https://n.news.naver.com/mnews/article/586/000...      0  \n",
              "4   https://n.news.naver.com/mnews/article/277/000...      0  \n",
              "5   https://n.news.naver.com/mnews/article/001/001...      0  \n",
              "6   https://n.news.naver.com/mnews/article/586/000...      0  \n",
              "7   https://n.news.naver.com/mnews/article/021/000...      0  \n",
              "8   https://n.news.naver.com/mnews/article/421/000...      0  \n",
              "9   https://n.news.naver.com/mnews/article/052/000...      0  \n",
              "10  https://entertain.naver.com/now/read?oid=312&a...      0  \n",
              "11  https://entertain.naver.com/now/read?oid=311&a...      0  \n",
              "12  https://entertain.naver.com/now/read?oid=213&a...      0  \n",
              "13  https://entertain.naver.com/now/read?oid=003&a...      0  \n",
              "14  https://entertain.naver.com/now/read?oid=416&a...      0  \n",
              "15  https://entertain.naver.com/now/read?oid=445&a...      0  \n",
              "16  https://entertain.naver.com/now/read?oid=445&a...      0  \n",
              "17  https://entertain.naver.com/now/read?oid=415&a...      0  \n",
              "18  https://entertain.naver.com/now/read?oid=445&a...      0  \n",
              "19  https://entertain.naver.com/now/read?oid=445&a...      0  \n",
              "20  https://sports.news.naver.com/news?oid=005&aid...      0  \n",
              "21  https://sports.news.naver.com/news?oid=001&aid...      0  \n",
              "22  https://sports.news.naver.com/news?oid=117&aid...      0  \n",
              "23  https://sports.news.naver.com/news?oid=370&aid...      0  \n",
              "24  https://sports.news.naver.com/news?oid=021&aid...      0  \n",
              "25  https://sports.news.naver.com/news?oid=003&aid...      0  \n",
              "26  https://sports.news.naver.com/news?oid=445&aid...      0  \n",
              "27  https://sports.news.naver.com/news?oid=425&aid...      0  \n",
              "28  https://sports.news.naver.com/news?oid=079&aid...      0  \n",
              "29  https://sports.news.naver.com/news?oid=481&aid...      0  "
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# insert(summary_article.values.tolist())\n",
        "select()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiH75fuJQei9"
      },
      "outputs": [],
      "source": [
        "# csv로 저장\n",
        "# article_df.to_csv(\"test.csv\",index=False, encoding=\"utf-8-sig\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
